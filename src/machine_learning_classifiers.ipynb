{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pvashisth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, metrics, naive_bayes, svm\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "\n",
    "import textblob, string, xgboost\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = \"C:\\\\Users\\\\pvashisth\\\\Desktop\\\\ai_course\\\\Git\\\\twitter-gender-classification\\\\src\\\\Data\\\\pos\"\n",
    "neg_path = \"C:\\\\Users\\\\pvashisth\\\\Desktop\\\\ai_course\\\\Git\\\\twitter-gender-classification\\\\src\\\\Data\\\\neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [] # text feature\n",
    "target = [] #labels to predict ml result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = [] # male\n",
    "neg_data = [] # female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in listdir(pos_path):\n",
    "    with open(join(pos_path, file), 'r', encoding='utf-8') as r:\n",
    "        d = r.read()\n",
    "        data.append(d)\n",
    "        target.append(1)\n",
    "        \n",
    "        pos_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9762, 9762, 9762)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(target), len(pos_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in listdir(neg_path):\n",
    "    with open(join(neg_path, file), 'r', encoding='utf-8') as r:\n",
    "        d = r.read()\n",
    "        data.append(d)\n",
    "        target.append(0)\n",
    "        \n",
    "        neg_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21021, 21021, 11259)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(target), len(neg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 9762, 0: 11259})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning - Base line (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16816"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16816, 12778)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(solver='sag', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.fit(tokens, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4205, 12778)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4205"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_model.predict(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.55049355, 0.51304901]),\n",
       " array([0.65462754, 0.40502513]),\n",
       " array([0.59806146, 0.45268183]),\n",
       " array([2215, 1990], dtype=int64))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision but we have taken the accuracy measures becuase dataset is balanced\n",
    "precision_recall_fscore_support(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5365041617122474"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP) - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.fit(tokens, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp_model.predict(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.51427212, 0.46077033]),\n",
       " array([0.48803612, 0.48693467]),\n",
       " array([0.50081075, 0.47349133]),\n",
       " array([2215, 1990], dtype=int64))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4875148632580262"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.fit(tokens, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = test_tokens, x_train = tokens\n",
    "\n",
    "predictions = svm_model.predict(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.52675386, 0.        ]),\n",
       " array([1., 0.]),\n",
       " array([0.69003115, 0.        ]),\n",
       " array([2215, 1990], dtype=int64))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5267538644470868"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusable code from Analytics Vidya\n",
    "#https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False, batch_size=32):\n",
    "    # fit the training dataset on the classifier\n",
    "    if is_neural_net == False:\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "    else:\n",
    "        classifier.fit(feature_vector_train, label, batch_size=batch_size)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = [int(round(p[0])) for p in predictions]\n",
    "    \n",
    "    return metrics.accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF:  0.5384066587395957\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), tokens, Y_train, test_tokens)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF:  0.47705112960761\n"
     ]
    }
   ],
   "source": [
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), tokens, Y_train, test_tokens)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.549346016646849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), tokens.tocsc(), Y_train, test_tokens.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings - W2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"C:\\\\Users\\\\pvashisth\\\\Downloads\\\\GoogleNewsVectors\\\\GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.syn0norm[wv.vocab[\"men\"].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314538"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kd nuggets ref\n",
    "#https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        #raise Exception(\"All words in a sentence not present in word2vec vocabulary.\")\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_sentences(wv, sentences):\n",
    "    print(\"Total sentences: {0}\".format(len(sentences)))\n",
    "    \n",
    "    avgd_vectors = []\n",
    "    for sentence in sentences:\n",
    "        avgd_vector = word_averaging(wv, sentence)\n",
    "        avgd_vectors.append(avgd_vector)\n",
    "        \n",
    "    print(\"Total converted: {0}\".format(len(avgd_vectors)))\n",
    "    return np.vstack(avgd_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kdnuggets.com/2018/11/multi-class-text-classification-model-comparison-selection.html/2\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = []\n",
    "X_test_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16816, 4205, 16816, 4205)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for xtr in X_train:\n",
    "    X_train_tokenized.append(w2v_tokenize_text(xtr))\n",
    "\n",
    "for xte in X_test:\n",
    "    X_test_tokenized.append(w2v_tokenize_text(xte))\n",
    "\n",
    "len(X_train_tokenized), len(X_test_tokenized), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Klopp',\n",
       "  '``',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'good',\n",
       "  'to',\n",
       "  'be',\n",
       "  'back',\n",
       "  'asked',\n",
       "  'for',\n",
       "  'pretzels',\n",
       "  'for',\n",
       "  'dinner',\n",
       "  'because',\n",
       "  'you',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'really',\n",
       "  'get',\n",
       "  'them',\n",
       "  'in',\n",
       "  'England',\n",
       "  \"''\"],\n",
       " ['Melbourne',\n",
       "  'Showtime',\n",
       "  \"'ve\",\n",
       "  'never',\n",
       "  'been',\n",
       "  'so',\n",
       "  'exhausted',\n",
       "  'before',\n",
       "  'but',\n",
       "  \"'m\",\n",
       "  'still',\n",
       "  'determined',\n",
       "  'to',\n",
       "  'be',\n",
       "  'as',\n",
       "  'fun',\n",
       "  'and',\n",
       "  'entertaining',\n",
       "  'as',\n",
       "  'p…'],\n",
       " ['honestly',\n",
       "  'find',\n",
       "  'this',\n",
       "  'fine',\n",
       "  'when',\n",
       "  'someone',\n",
       "  'does',\n",
       "  'it',\n",
       "  'it',\n",
       "  'not',\n",
       "  'like',\n",
       "  'you',\n",
       "  'can',\n",
       "  'spend',\n",
       "  'everyday',\n",
       "  'talking',\n",
       "  'to',\n",
       "  'everyone..',\n",
       "  'you',\n",
       "  'need',\n",
       "  'to',\n",
       "  'kn…'],\n",
       " ['The',\n",
       "  'only',\n",
       "  'reason',\n",
       "  \"'ve\",\n",
       "  'been',\n",
       "  'going',\n",
       "  'out',\n",
       "  'with',\n",
       "  'this',\n",
       "  'guy',\n",
       "  'all',\n",
       "  'summer',\n",
       "  'is',\n",
       "  'because',\n",
       "  'have',\n",
       "  'no',\n",
       "  'idea',\n",
       "  'how',\n",
       "  'to',\n",
       "  'operate',\n",
       "  'my',\n",
       "  'gas',\n",
       "  'grill'],\n",
       " ['Thanks',\n",
       "  'also',\n",
       "  'to',\n",
       "  'Dr',\n",
       "  'amp',\n",
       "  'for',\n",
       "  'looking',\n",
       "  'at',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'coconuts',\n",
       "  'for',\n",
       "  'drilling',\n",
       "  'and',\n",
       "  'prevention',\n",
       "  'of',\n",
       "  'CSF',\n",
       "  'l…'],\n",
       " ['Europe',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'absolutely',\n",
       "  'amazing',\n",
       "  'to',\n",
       "  'us',\n",
       "  \"'m\",\n",
       "  'sure',\n",
       "  'we',\n",
       "  'will',\n",
       "  'see',\n",
       "  'all',\n",
       "  'very',\n",
       "  'soon',\n",
       "  'Thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'making',\n",
       "  'my',\n",
       "  'dreams',\n",
       "  'come…'],\n",
       " ['have',\n",
       "  'embraced',\n",
       "  'many',\n",
       "  'beautiful',\n",
       "  'memories',\n",
       "  'in',\n",
       "  'my',\n",
       "  'life',\n",
       "  'but',\n",
       "  'the',\n",
       "  'one',\n",
       "  'admire',\n",
       "  'the',\n",
       "  'most',\n",
       "  'was',\n",
       "  'when',\n",
       "  'met',\n",
       "  'you'],\n",
       " ['Getting',\n",
       "  'your',\n",
       "  'shit',\n",
       "  'together',\n",
       "  'requires',\n",
       "  'level',\n",
       "  'of',\n",
       "  'honesty',\n",
       "  'you',\n",
       "  'can',\n",
       "  'even',\n",
       "  'imagine',\n",
       "  'There',\n",
       "  'nothing',\n",
       "  'easy',\n",
       "  'about',\n",
       "  'realizing',\n",
       "  'you',\n",
       "  're',\n",
       "  'th…'],\n",
       " ['Are',\n",
       "  'you',\n",
       "  'dangerously',\n",
       "  'out',\n",
       "  'of',\n",
       "  'touch',\n",
       "  'with',\n",
       "  'your',\n",
       "  'customers',\n",
       "  '️Our',\n",
       "  'report',\n",
       "  'with',\n",
       "  'found',\n",
       "  'that',\n",
       "  'of',\n",
       "  'business',\n",
       "  'leaders',\n",
       "  'ar…'],\n",
       " ['Did',\n",
       "  'you',\n",
       "  'hear',\n",
       "  'Yorkdale',\n",
       "  'is',\n",
       "  'going',\n",
       "  'to',\n",
       "  'double',\n",
       "  'in',\n",
       "  'size',\n",
       "  'by',\n",
       "  'August',\n",
       "  'In',\n",
       "  'the',\n",
       "  'works',\n",
       "  ',000-square-fo',\n",
       "  '...'],\n",
       " ['Did',\n",
       "  'you',\n",
       "  'know',\n",
       "  'if',\n",
       "  'you',\n",
       "  'scream',\n",
       "  '``',\n",
       "  'Bloody',\n",
       "  'Mary',\n",
       "  \"''\",\n",
       "  'times',\n",
       "  'in',\n",
       "  'the',\n",
       "  'mirror',\n",
       "  'at',\n",
       "  'AM',\n",
       "  'your',\n",
       "  'mom',\n",
       "  'will',\n",
       "  'tell',\n",
       "  'you',\n",
       "  'to',\n",
       "  'be',\n",
       "  'quiet',\n",
       "  'and',\n",
       "  'go',\n",
       "  'to',\n",
       "  'bed'],\n",
       " ['After',\n",
       "  'last',\n",
       "  'week',\n",
       "  'this',\n",
       "  'week',\n",
       "  'has',\n",
       "  'got',\n",
       "  'to',\n",
       "  'be',\n",
       "  'better',\n",
       "  'spent',\n",
       "  'few',\n",
       "  'days',\n",
       "  'in',\n",
       "  'hospital',\n",
       "  'with',\n",
       "  'ruptured',\n",
       "  'cyst',\n",
       "  'and',\n",
       "  'internal',\n",
       "  'bl…',\n",
       "  'h…'],\n",
       " ['Starting',\n",
       "  'petition',\n",
       "  'to',\n",
       "  'get',\n",
       "  'Mets',\n",
       "  'Jeff',\n",
       "  'McNeil',\n",
       "  'to',\n",
       "  'change',\n",
       "  'his',\n",
       "  'walk-up',\n",
       "  'music',\n",
       "  'to',\n",
       "  '``',\n",
       "  'Atomic',\n",
       "  'Dog',\n",
       "  \"''\",\n",
       "  'since',\n",
       "  'he',\n",
       "  'homered',\n",
       "  'trying',\n",
       "  'to…'],\n",
       " ['Sometimes',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'hard',\n",
       "  'to',\n",
       "  'tell',\n",
       "  'whether',\n",
       "  'seeing',\n",
       "  'the',\n",
       "  'bright',\n",
       "  'side',\n",
       "  'of',\n",
       "  'everything',\n",
       "  'makes',\n",
       "  'you',\n",
       "  'too',\n",
       "  'optimistic',\n",
       "  'irrealistic',\n",
       "  'of',\n",
       "  'too',\n",
       "  'naive'],\n",
       " ['You',\n",
       "  'anit',\n",
       "  'even',\n",
       "  'cute',\n",
       "  'yo',\n",
       "  'fucc',\n",
       "  'up',\n",
       "  'assets',\n",
       "  'body',\n",
       "  'shit',\n",
       "  'Bitch',\n",
       "  'you',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'even',\n",
       "  'see',\n",
       "  'yo',\n",
       "  'pussy',\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'how',\n",
       "  'fucc',\n",
       "  'yo',\n",
       "  'shit',\n",
       "  'it'],\n",
       " ['just',\n",
       "  'drank',\n",
       "  'this',\n",
       "  'cleansing',\n",
       "  'juice',\n",
       "  'that',\n",
       "  'tastes',\n",
       "  'like',\n",
       "  'milkshake',\n",
       "  'omg',\n",
       "  'so',\n",
       "  'much',\n",
       "  'better',\n",
       "  'than',\n",
       "  'the',\n",
       "  'foot',\n",
       "  'juice',\n",
       "  'had',\n",
       "  'yesterday'],\n",
       " ['would',\n",
       "  'donate',\n",
       "  'some',\n",
       "  'money',\n",
       "  'to',\n",
       "  'the',\n",
       "  'children',\n",
       "  \"'s\",\n",
       "  'hospital',\n",
       "  'in',\n",
       "  'Glasgow',\n",
       "  'in',\n",
       "  'memory',\n",
       "  'of',\n",
       "  'my',\n",
       "  'daughter',\n",
       "  'and',\n",
       "  'some',\n",
       "  'to',\n",
       "  'm…'],\n",
       " ['Live',\n",
       "  'from',\n",
       "  'We',\n",
       "  'caught',\n",
       "  'up',\n",
       "  'with',\n",
       "  'Suzan',\n",
       "  'from',\n",
       "  'the',\n",
       "  'State',\n",
       "  'of',\n",
       "  'Arizona',\n",
       "  'after',\n",
       "  'our',\n",
       "  'panel',\n",
       "  'discussion',\n",
       "  'on',\n",
       "  'enterprise',\n",
       "  'cl…'],\n",
       " ['have',\n",
       "  'embraced',\n",
       "  'many',\n",
       "  'beautiful',\n",
       "  'memories',\n",
       "  'in',\n",
       "  'my',\n",
       "  'life',\n",
       "  'but',\n",
       "  'the',\n",
       "  'one',\n",
       "  'admire',\n",
       "  'the',\n",
       "  'most',\n",
       "  'was',\n",
       "  'when',\n",
       "  'met',\n",
       "  'you'],\n",
       " ['Tell',\n",
       "  'that',\n",
       "  'to',\n",
       "  'the',\n",
       "  'people',\n",
       "  'operating',\n",
       "  'the',\n",
       "  'live',\n",
       "  'chat',\n",
       "  \"'ve\",\n",
       "  'just',\n",
       "  'spoke',\n",
       "  'to',\n",
       "  'somebody',\n",
       "  'again',\n",
       "  'that',\n",
       "  'said',\n",
       "  'my',\n",
       "  'money',\n",
       "  'will',\n",
       "  'b…'],\n",
       " ['wanted',\n",
       "  'to',\n",
       "  'celebrate',\n",
       "  'reaching',\n",
       "  'followers',\n",
       "  'so',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'ask',\n",
       "  'mates',\n",
       "  'to',\n",
       "  'team',\n",
       "  'up',\n",
       "  'with',\n",
       "  'me',\n",
       "  'for',\n",
       "  '-fold',\n",
       "  'amp',\n",
       "  'whacked',\n",
       "  '£60',\n",
       "  'on',\n",
       "  'i…'],\n",
       " ['was',\n",
       "  'wearing',\n",
       "  'fluffy',\n",
       "  'socks',\n",
       "  'and',\n",
       "  'yeah',\n",
       "  'in',\n",
       "  'the',\n",
       "  'group',\n",
       "  'we',\n",
       "  'have',\n",
       "  'at',\n",
       "  'the',\n",
       "  'moment',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'like',\n",
       "  'boys',\n",
       "  'and',\n",
       "  'girls',\n",
       "  'it',\n",
       "  'was',\n",
       "  'outside',\n",
       "  'hockey'],\n",
       " ['Doing',\n",
       "  'anything',\n",
       "  'this',\n",
       "  'week',\n",
       "  'or',\n",
       "  'next',\n",
       "  'Well',\n",
       "  'my',\n",
       "  'auntie',\n",
       "  'and',\n",
       "  'great',\n",
       "  'nanny',\n",
       "  'are',\n",
       "  'coming',\n",
       "  'down',\n",
       "  'tomorrow',\n",
       "  'untill',\n",
       "  'Thursday',\n",
       "  'the',\n",
       "  '...'],\n",
       " ['feel',\n",
       "  'so',\n",
       "  'sorry',\n",
       "  'for',\n",
       "  'Mitt',\n",
       "  'Romney',\n",
       "  'but',\n",
       "  'sorrier',\n",
       "  'for',\n",
       "  'the',\n",
       "  'country',\n",
       "  'that',\n",
       "  'will',\n",
       "  'never',\n",
       "  'have',\n",
       "  'him',\n",
       "  'as',\n",
       "  'president'],\n",
       " ['to',\n",
       "  'my',\n",
       "  'mother',\n",
       "  'may',\n",
       "  'every',\n",
       "  'tear',\n",
       "  'that',\n",
       "  'has',\n",
       "  'fallen',\n",
       "  'from',\n",
       "  'your',\n",
       "  'tired',\n",
       "  'eyes',\n",
       "  'on',\n",
       "  'my',\n",
       "  'behalf',\n",
       "  'become',\n",
       "  'river',\n",
       "  'for',\n",
       "  'you',\n",
       "  'in',\n",
       "  'Paradise️'],\n",
       " ['haha',\n",
       "  'good',\n",
       "  'idea',\n",
       "  'but',\n",
       "  'am',\n",
       "  'the',\n",
       "  'worse',\n",
       "  'singer',\n",
       "  'on',\n",
       "  'the',\n",
       "  'planet',\n",
       "  'earth',\n",
       "  'can',\n",
       "  'play',\n",
       "  'the',\n",
       "  'kazoo',\n",
       "  'though'],\n",
       " ['Last',\n",
       "  'night',\n",
       "  'we',\n",
       "  'met',\n",
       "  'very',\n",
       "  'brave',\n",
       "  'little',\n",
       "  'dude',\n",
       "  'called',\n",
       "  'Gavin',\n",
       "  'Gavin',\n",
       "  'in',\n",
       "  'an',\n",
       "  'old',\n",
       "  'head',\n",
       "  'on',\n",
       "  'young',\n",
       "  'shoulders',\n",
       "  'so',\n",
       "  'much',\n",
       "  'fun',\n",
       "  'teaching',\n",
       "  'u…'],\n",
       " ['national',\n",
       "  'anthem',\n",
       "  'is',\n",
       "  'also',\n",
       "  'very',\n",
       "  'sensual',\n",
       "  '``',\n",
       "  'sing',\n",
       "  'the',\n",
       "  'national',\n",
       "  'anthem',\n",
       "  'while',\n",
       "  \"'m\",\n",
       "  'standing',\n",
       "  'over',\n",
       "  'your',\n",
       "  'body',\n",
       "  'hold',\n",
       "  'you',\n",
       "  'like',\n",
       "  'python',\n",
       "  \"''\"],\n",
       " ['Family',\n",
       "  'food',\n",
       "  'games',\n",
       "  'ear',\n",
       "  'defenders',\n",
       "  'My',\n",
       "  'autistic',\n",
       "  'son',\n",
       "  'does',\n",
       "  \"n't\",\n",
       "  'mind',\n",
       "  'groups',\n",
       "  'as',\n",
       "  'long',\n",
       "  'as',\n",
       "  'he',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'hear',\n",
       "  'them',\n",
       "  'there',\n",
       "  'is',\n",
       "  'food'],\n",
       " ['Yes',\n",
       "  'this',\n",
       "  'tweet',\n",
       "  'might',\n",
       "  'have',\n",
       "  ',000',\n",
       "  'retweets',\n",
       "  'but',\n",
       "  'one',\n",
       "  'of',\n",
       "  'them',\n",
       "  'is',\n",
       "  'chandler',\n",
       "  'and',\n",
       "  'he',\n",
       "  'never',\n",
       "  'wins',\n",
       "  'So',\n",
       "  'go',\n",
       "  'retweet',\n",
       "  'if',\n",
       "  'you',\n",
       "  'haven',\n",
       "  'caus…'],\n",
       " ['Do',\n",
       "  'you',\n",
       "  'have',\n",
       "  'large',\n",
       "  'crew',\n",
       "  'on-site',\n",
       "  'completing',\n",
       "  'project',\n",
       "  'Make',\n",
       "  'sure',\n",
       "  'to',\n",
       "  'call',\n",
       "  'Andy',\n",
       "  'Gump',\n",
       "  'at',\n",
       "  '-992-7755',\n",
       "  'to',\n",
       "  'provide',\n",
       "  'the',\n",
       "  '...'],\n",
       " ['This',\n",
       "  'guy',\n",
       "  'probably',\n",
       "  'realized',\n",
       "  'what',\n",
       "  'gem',\n",
       "  'of',\n",
       "  'woman',\n",
       "  'he',\n",
       "  'was',\n",
       "  'about',\n",
       "  'to',\n",
       "  'lose',\n",
       "  'so',\n",
       "  'being',\n",
       "  'little',\n",
       "  'fuckboy',\n",
       "  'to',\n",
       "  'save',\n",
       "  'himself',\n",
       "  'th…'],\n",
       " ['remember',\n",
       "  'when',\n",
       "  'Rupaul',\n",
       "  'at',\n",
       "  'season',\n",
       "  \"'s\",\n",
       "  'finale',\n",
       "  'said',\n",
       "  'to',\n",
       "  'Monica',\n",
       "  'Beverly',\n",
       "  'Hillz',\n",
       "  'that',\n",
       "  'to',\n",
       "  'be',\n",
       "  'America',\n",
       "  \"'s\",\n",
       "  'next',\n",
       "  'drag',\n",
       "  'super',\n",
       "  'star…'],\n",
       " ['what',\n",
       "  'do',\n",
       "  'Darren',\n",
       "  'Helm',\n",
       "  'and',\n",
       "  'Thomas',\n",
       "  'Stuart',\n",
       "  'Dant',\n",
       "  'have',\n",
       "  'in',\n",
       "  'common',\n",
       "  'Only',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Jr',\n",
       "  'Selkirk',\n",
       "  'Fishermen',\n",
       "  'to',\n",
       "  'sign',\n",
       "  'pro',\n",
       "  'deals'],\n",
       " ['Just',\n",
       "  'start',\n",
       "  'doing',\n",
       "  'what',\n",
       "  'you',\n",
       "  'know',\n",
       "  'is',\n",
       "  'the',\n",
       "  'right',\n",
       "  'thing',\n",
       "  'to',\n",
       "  'do',\n",
       "  'and',\n",
       "  'the',\n",
       "  'feelings',\n",
       "  'will',\n",
       "  'follow',\n",
       "  'Do',\n",
       "  \"n't\",\n",
       "  'wait.Action',\n",
       "  'ignites',\n",
       "  'motivation'],\n",
       " ['heard',\n",
       "  'sirens',\n",
       "  'in',\n",
       "  'Matlock',\n",
       "  'area',\n",
       "  'mid',\n",
       "  'afternoon',\n",
       "  'an',\n",
       "  'hour',\n",
       "  'or',\n",
       "  'so',\n",
       "  'later',\n",
       "  'Hercules',\n",
       "  'flying',\n",
       "  'back',\n",
       "  'and',\n",
       "  'forth',\n",
       "  'along',\n",
       "  'Dunnottar',\n",
       "  'shoreline'],\n",
       " ['Your',\n",
       "  'favorite',\n",
       "  'teacher',\n",
       "  'band',\n",
       "  'will',\n",
       "  'be',\n",
       "  'playing',\n",
       "  'at',\n",
       "  ':45',\n",
       "  'tomorrow',\n",
       "  'night',\n",
       "  'at',\n",
       "  'Loosey',\n",
       "  \"'s\",\n",
       "  'Open',\n",
       "  'Mic',\n",
       "  'Night',\n",
       "  'Come',\n",
       "  'out',\n",
       "  'and',\n",
       "  'show',\n",
       "  'some',\n",
       "  'love'],\n",
       " ['Yesterday',\n",
       "  'was',\n",
       "  'the',\n",
       "  'best',\n",
       "  'day',\n",
       "  'of',\n",
       "  'my',\n",
       "  'LIFE',\n",
       "  'Took',\n",
       "  'my',\n",
       "  'Shahada',\n",
       "  'and',\n",
       "  'became',\n",
       "  'Muslim',\n",
       "  'want',\n",
       "  'to',\n",
       "  'thank',\n",
       "  'everyone',\n",
       "  'that',\n",
       "  'was',\n",
       "  'apart',\n",
       "  'of',\n",
       "  'this…'],\n",
       " ['omg',\n",
       "  'aht',\n",
       "  'time',\n",
       "  'my',\n",
       "  'boyfriend',\n",
       "  'is',\n",
       "  'soo',\n",
       "  'needy',\n",
       "  'just',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'punch',\n",
       "  'home',\n",
       "  'but',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'help',\n",
       "  'it',\n",
       "  'but',\n",
       "  'to',\n",
       "  'love',\n",
       "  'him'],\n",
       " ['Micky',\n",
       "  'giving',\n",
       "  'you',\n",
       "  'small',\n",
       "  'locket',\n",
       "  'before',\n",
       "  'leaving',\n",
       "  'to',\n",
       "  'tour',\n",
       "  'the',\n",
       "  'picture',\n",
       "  'inside',\n",
       "  'is',\n",
       "  'of',\n",
       "  'you',\n",
       "  'two',\n",
       "  'the',\n",
       "  'day',\n",
       "  'you',\n",
       "  'met'],\n",
       " ['have',\n",
       "  'just',\n",
       "  'small',\n",
       "  'desk',\n",
       "  'on',\n",
       "  'one',\n",
       "  'wall',\n",
       "  'for',\n",
       "  'my',\n",
       "  'old',\n",
       "  'computer',\n",
       "  'twin',\n",
       "  'bed',\n",
       "  'parked',\n",
       "  'against',\n",
       "  'the',\n",
       "  'far',\n",
       "  'wall'],\n",
       " ['told',\n",
       "  'my',\n",
       "  'cat',\n",
       "  'she',\n",
       "  'wasn',\n",
       "  'gon',\n",
       "  'na',\n",
       "  'like',\n",
       "  'this',\n",
       "  'salt',\n",
       "  'and',\n",
       "  'vinegar',\n",
       "  'chip',\n",
       "  'but',\n",
       "  'she',\n",
       "  'didn',\n",
       "  'listen',\n",
       "  'to',\n",
       "  'me',\n",
       "  'smh',\n",
       "  'SHE',\n",
       "  'GAGGED'],\n",
       " ['dont',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'go',\n",
       "  'to',\n",
       "  'school',\n",
       "  'but',\n",
       "  'think',\n",
       "  'might',\n",
       "  'have',\n",
       "  'to',\n",
       "  '...',\n",
       "  'so',\n",
       "  'much',\n",
       "  'on',\n",
       "  'my',\n",
       "  'mind',\n",
       "  'cant',\n",
       "  'deal'],\n",
       " ['So',\n",
       "  'Elizabeth',\n",
       "  'Warren',\n",
       "  'has',\n",
       "  'the',\n",
       "  'stamina',\n",
       "  'to',\n",
       "  'take',\n",
       "  'all',\n",
       "  'those',\n",
       "  'selfies',\n",
       "  'and',\n",
       "  'still',\n",
       "  'have',\n",
       "  'energy',\n",
       "  'to',\n",
       "  'have',\n",
       "  'an',\n",
       "  'affair',\n",
       "  'with',\n",
       "  'year',\n",
       "  'old…'],\n",
       " ['If',\n",
       "  'bride',\n",
       "  'does',\n",
       "  \"n't\",\n",
       "  'wear',\n",
       "  'false',\n",
       "  'eyelashes',\n",
       "  'at',\n",
       "  'her',\n",
       "  'wedding',\n",
       "  'the',\n",
       "  'marriage',\n",
       "  'will',\n",
       "  'be',\n",
       "  'complete',\n",
       "  'amp',\n",
       "  'utter',\n",
       "  'failure'],\n",
       " ['Micky',\n",
       "  'and',\n",
       "  'you',\n",
       "  'cooking',\n",
       "  'it',\n",
       "  'actually',\n",
       "  'comes',\n",
       "  'out',\n",
       "  'really',\n",
       "  'well',\n",
       "  'but',\n",
       "  'when',\n",
       "  'your',\n",
       "  'finally',\n",
       "  'finished',\n",
       "  'you',\n",
       "  'both',\n",
       "  'are',\n",
       "  \"n't\",\n",
       "  'hungry',\n",
       "  'anymore'],\n",
       " ['It',\n",
       "  \"'s\",\n",
       "  'Friday',\n",
       "  'and',\n",
       "  'time',\n",
       "  'to',\n",
       "  'plan',\n",
       "  'relaxing',\n",
       "  'weekend',\n",
       "  'Try',\n",
       "  'Having',\n",
       "  'lie-in',\n",
       "  'Getting',\n",
       "  'some',\n",
       "  'fresh',\n",
       "  'air',\n",
       "  'in',\n",
       "  'the',\n",
       "  'house…'],\n",
       " ['Tell',\n",
       "  'lay',\n",
       "  'said',\n",
       "  'HELLO',\n",
       "  'amp',\n",
       "  'Dede',\n",
       "  'is',\n",
       "  'she',\n",
       "  'chill',\n",
       "  'now',\n",
       "  'that',\n",
       "  'she',\n",
       "  'got',\n",
       "  'some',\n",
       "  'weed',\n",
       "  'in',\n",
       "  'her',\n",
       "  'system',\n",
       "  'But',\n",
       "  \"'m\",\n",
       "  'in',\n",
       "  'palm',\n",
       "  'desert',\n",
       "  'again'],\n",
       " ['Government',\n",
       "  'plans',\n",
       "  'will',\n",
       "  'starve',\n",
       "  'injured',\n",
       "  'people',\n",
       "  'of',\n",
       "  'access',\n",
       "  'to',\n",
       "  'justice',\n",
       "  'and',\n",
       "  'take',\n",
       "  'at',\n",
       "  'least',\n",
       "  '£135m',\n",
       "  'from',\n",
       "  'public',\n",
       "  'funds',\n",
       "  'Act',\n",
       "  'now',\n",
       "  'ht…'],\n",
       " [\"'no\",\n",
       "  'Dan',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'want',\n",
       "  'to',\n",
       "  'go',\n",
       "  'on',\n",
       "  'this',\n",
       "  'one',\n",
       "  'you',\n",
       "  'scream',\n",
       "  'at',\n",
       "  'him',\n",
       "  'as',\n",
       "  'he',\n",
       "  'drags',\n",
       "  'you',\n",
       "  'in',\n",
       "  'the',\n",
       "  'short',\n",
       "  'queue',\n",
       "  'of',\n",
       "  'the',\n",
       "  'cont']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 16816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total converted: 16816\n",
      "Total sentences: 4205\n",
      "Total converted: 4205\n"
     ]
    }
   ],
   "source": [
    "X_train_avg = word_averaging_sentences(wv, X_train_tokenized)\n",
    "X_test_avg = word_averaging_sentences(wv, X_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5443519619500594\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='sag', max_iter=1000)\n",
    "logreg = logreg.fit(X_train_avg, Y_train)\n",
    "y_pred = logreg.predict(X_test_avg)\n",
    "print('accuracy %s' % accuracy_score(y_pred, Y_test))\n",
    "#print(classification_report(Y_test, y_pred, target_names=['female', 'male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5267538644470868"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVC()\n",
    "\n",
    "svm_model.fit(X_train_avg, Y_train)\n",
    "\n",
    "predictions = svm_model.predict(X_test_avg)\n",
    "\n",
    "precision_recall_fscore_support(Y_test, predictions)\n",
    "\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusable code from Analytics Vidya\n",
    "#https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False, batch_size=32):\n",
    "    # fit the training dataset on the classifier\n",
    "    if is_neural_net == False:\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "    else:\n",
    "        classifier.fit(feature_vector_train, label, batch_size=batch_size)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = [int(round(p[0])) for p in predictions]\n",
    "    \n",
    "    return metrics.accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF:  0.47728894173602854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RF on Word Level on Word embedding (W2Vec)\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), X_train_avg, Y_train, X_test_avg)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.5538644470868014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Word embedding (W2Vec)\n",
    "accuracy = train_model(xgboost.XGBClassifier(), X_train_avg, Y_train, X_test_avg)\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings - Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1045: RuntimeWarning: overflow encountered in square\n",
      "  self.vectors[i, :] /= sqrt((self.vectors[i, :] ** 2).sum(-1))\n",
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:38: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"C:\\\\Users\\\\pvashisth\\\\Downloads\\\\glove-twitter-200.gz\", binary=True, encoding='latin-1')\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314538"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        #raise Exception(\"All words in a sentence not present in word2vec vocabulary.\")\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_sentences(wv, sentences):\n",
    "    print(\"Total sentences: {0}\".format(len(sentences)))\n",
    "    \n",
    "    avgd_vectors = []\n",
    "    for sentence in sentences:\n",
    "        avgd_vector = word_averaging(wv, sentence)\n",
    "        avgd_vectors.append(avgd_vector)\n",
    "        \n",
    "    print(\"Total converted: {0}\".format(len(avgd_vectors)))\n",
    "    return np.vstack(avgd_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.syn0norm[wv.vocab[\"man\"].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = []\n",
    "X_test_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16816, 4205, 16816, 4205)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for xtr in X_train:\n",
    "    X_train_tokenized.append(w2v_tokenize_text(xtr))\n",
    "\n",
    "for xte in X_test:\n",
    "    X_test_tokenized.append(w2v_tokenize_text(xte))\n",
    "\n",
    "len(X_train_tokenized), len(X_test_tokenized), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Klopp',\n",
       "  '``',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'good',\n",
       "  'to',\n",
       "  'be',\n",
       "  'back',\n",
       "  'asked',\n",
       "  'for',\n",
       "  'pretzels',\n",
       "  'for',\n",
       "  'dinner',\n",
       "  'because',\n",
       "  'you',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'really',\n",
       "  'get',\n",
       "  'them',\n",
       "  'in',\n",
       "  'England',\n",
       "  \"''\"],\n",
       " ['Melbourne',\n",
       "  'Showtime',\n",
       "  \"'ve\",\n",
       "  'never',\n",
       "  'been',\n",
       "  'so',\n",
       "  'exhausted',\n",
       "  'before',\n",
       "  'but',\n",
       "  \"'m\",\n",
       "  'still',\n",
       "  'determined',\n",
       "  'to',\n",
       "  'be',\n",
       "  'as',\n",
       "  'fun',\n",
       "  'and',\n",
       "  'entertaining',\n",
       "  'as',\n",
       "  'p…'],\n",
       " ['honestly',\n",
       "  'find',\n",
       "  'this',\n",
       "  'fine',\n",
       "  'when',\n",
       "  'someone',\n",
       "  'does',\n",
       "  'it',\n",
       "  'it',\n",
       "  'not',\n",
       "  'like',\n",
       "  'you',\n",
       "  'can',\n",
       "  'spend',\n",
       "  'everyday',\n",
       "  'talking',\n",
       "  'to',\n",
       "  'everyone..',\n",
       "  'you',\n",
       "  'need',\n",
       "  'to',\n",
       "  'kn…'],\n",
       " ['The',\n",
       "  'only',\n",
       "  'reason',\n",
       "  \"'ve\",\n",
       "  'been',\n",
       "  'going',\n",
       "  'out',\n",
       "  'with',\n",
       "  'this',\n",
       "  'guy',\n",
       "  'all',\n",
       "  'summer',\n",
       "  'is',\n",
       "  'because',\n",
       "  'have',\n",
       "  'no',\n",
       "  'idea',\n",
       "  'how',\n",
       "  'to',\n",
       "  'operate',\n",
       "  'my',\n",
       "  'gas',\n",
       "  'grill'],\n",
       " ['Thanks',\n",
       "  'also',\n",
       "  'to',\n",
       "  'Dr',\n",
       "  'amp',\n",
       "  'for',\n",
       "  'looking',\n",
       "  'at',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'coconuts',\n",
       "  'for',\n",
       "  'drilling',\n",
       "  'and',\n",
       "  'prevention',\n",
       "  'of',\n",
       "  'CSF',\n",
       "  'l…'],\n",
       " ['Europe',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'absolutely',\n",
       "  'amazing',\n",
       "  'to',\n",
       "  'us',\n",
       "  \"'m\",\n",
       "  'sure',\n",
       "  'we',\n",
       "  'will',\n",
       "  'see',\n",
       "  'all',\n",
       "  'very',\n",
       "  'soon',\n",
       "  'Thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'making',\n",
       "  'my',\n",
       "  'dreams',\n",
       "  'come…'],\n",
       " ['have',\n",
       "  'embraced',\n",
       "  'many',\n",
       "  'beautiful',\n",
       "  'memories',\n",
       "  'in',\n",
       "  'my',\n",
       "  'life',\n",
       "  'but',\n",
       "  'the',\n",
       "  'one',\n",
       "  'admire',\n",
       "  'the',\n",
       "  'most',\n",
       "  'was',\n",
       "  'when',\n",
       "  'met',\n",
       "  'you'],\n",
       " ['Getting',\n",
       "  'your',\n",
       "  'shit',\n",
       "  'together',\n",
       "  'requires',\n",
       "  'level',\n",
       "  'of',\n",
       "  'honesty',\n",
       "  'you',\n",
       "  'can',\n",
       "  'even',\n",
       "  'imagine',\n",
       "  'There',\n",
       "  'nothing',\n",
       "  'easy',\n",
       "  'about',\n",
       "  'realizing',\n",
       "  'you',\n",
       "  're',\n",
       "  'th…'],\n",
       " ['Are',\n",
       "  'you',\n",
       "  'dangerously',\n",
       "  'out',\n",
       "  'of',\n",
       "  'touch',\n",
       "  'with',\n",
       "  'your',\n",
       "  'customers',\n",
       "  '️Our',\n",
       "  'report',\n",
       "  'with',\n",
       "  'found',\n",
       "  'that',\n",
       "  'of',\n",
       "  'business',\n",
       "  'leaders',\n",
       "  'ar…'],\n",
       " ['Did',\n",
       "  'you',\n",
       "  'hear',\n",
       "  'Yorkdale',\n",
       "  'is',\n",
       "  'going',\n",
       "  'to',\n",
       "  'double',\n",
       "  'in',\n",
       "  'size',\n",
       "  'by',\n",
       "  'August',\n",
       "  'In',\n",
       "  'the',\n",
       "  'works',\n",
       "  ',000-square-fo',\n",
       "  '...'],\n",
       " ['Did',\n",
       "  'you',\n",
       "  'know',\n",
       "  'if',\n",
       "  'you',\n",
       "  'scream',\n",
       "  '``',\n",
       "  'Bloody',\n",
       "  'Mary',\n",
       "  \"''\",\n",
       "  'times',\n",
       "  'in',\n",
       "  'the',\n",
       "  'mirror',\n",
       "  'at',\n",
       "  'AM',\n",
       "  'your',\n",
       "  'mom',\n",
       "  'will',\n",
       "  'tell',\n",
       "  'you',\n",
       "  'to',\n",
       "  'be',\n",
       "  'quiet',\n",
       "  'and',\n",
       "  'go',\n",
       "  'to',\n",
       "  'bed'],\n",
       " ['After',\n",
       "  'last',\n",
       "  'week',\n",
       "  'this',\n",
       "  'week',\n",
       "  'has',\n",
       "  'got',\n",
       "  'to',\n",
       "  'be',\n",
       "  'better',\n",
       "  'spent',\n",
       "  'few',\n",
       "  'days',\n",
       "  'in',\n",
       "  'hospital',\n",
       "  'with',\n",
       "  'ruptured',\n",
       "  'cyst',\n",
       "  'and',\n",
       "  'internal',\n",
       "  'bl…',\n",
       "  'h…'],\n",
       " ['Starting',\n",
       "  'petition',\n",
       "  'to',\n",
       "  'get',\n",
       "  'Mets',\n",
       "  'Jeff',\n",
       "  'McNeil',\n",
       "  'to',\n",
       "  'change',\n",
       "  'his',\n",
       "  'walk-up',\n",
       "  'music',\n",
       "  'to',\n",
       "  '``',\n",
       "  'Atomic',\n",
       "  'Dog',\n",
       "  \"''\",\n",
       "  'since',\n",
       "  'he',\n",
       "  'homered',\n",
       "  'trying',\n",
       "  'to…'],\n",
       " ['Sometimes',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'hard',\n",
       "  'to',\n",
       "  'tell',\n",
       "  'whether',\n",
       "  'seeing',\n",
       "  'the',\n",
       "  'bright',\n",
       "  'side',\n",
       "  'of',\n",
       "  'everything',\n",
       "  'makes',\n",
       "  'you',\n",
       "  'too',\n",
       "  'optimistic',\n",
       "  'irrealistic',\n",
       "  'of',\n",
       "  'too',\n",
       "  'naive'],\n",
       " ['You',\n",
       "  'anit',\n",
       "  'even',\n",
       "  'cute',\n",
       "  'yo',\n",
       "  'fucc',\n",
       "  'up',\n",
       "  'assets',\n",
       "  'body',\n",
       "  'shit',\n",
       "  'Bitch',\n",
       "  'you',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'even',\n",
       "  'see',\n",
       "  'yo',\n",
       "  'pussy',\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'how',\n",
       "  'fucc',\n",
       "  'yo',\n",
       "  'shit',\n",
       "  'it'],\n",
       " ['just',\n",
       "  'drank',\n",
       "  'this',\n",
       "  'cleansing',\n",
       "  'juice',\n",
       "  'that',\n",
       "  'tastes',\n",
       "  'like',\n",
       "  'milkshake',\n",
       "  'omg',\n",
       "  'so',\n",
       "  'much',\n",
       "  'better',\n",
       "  'than',\n",
       "  'the',\n",
       "  'foot',\n",
       "  'juice',\n",
       "  'had',\n",
       "  'yesterday'],\n",
       " ['would',\n",
       "  'donate',\n",
       "  'some',\n",
       "  'money',\n",
       "  'to',\n",
       "  'the',\n",
       "  'children',\n",
       "  \"'s\",\n",
       "  'hospital',\n",
       "  'in',\n",
       "  'Glasgow',\n",
       "  'in',\n",
       "  'memory',\n",
       "  'of',\n",
       "  'my',\n",
       "  'daughter',\n",
       "  'and',\n",
       "  'some',\n",
       "  'to',\n",
       "  'm…'],\n",
       " ['Live',\n",
       "  'from',\n",
       "  'We',\n",
       "  'caught',\n",
       "  'up',\n",
       "  'with',\n",
       "  'Suzan',\n",
       "  'from',\n",
       "  'the',\n",
       "  'State',\n",
       "  'of',\n",
       "  'Arizona',\n",
       "  'after',\n",
       "  'our',\n",
       "  'panel',\n",
       "  'discussion',\n",
       "  'on',\n",
       "  'enterprise',\n",
       "  'cl…'],\n",
       " ['have',\n",
       "  'embraced',\n",
       "  'many',\n",
       "  'beautiful',\n",
       "  'memories',\n",
       "  'in',\n",
       "  'my',\n",
       "  'life',\n",
       "  'but',\n",
       "  'the',\n",
       "  'one',\n",
       "  'admire',\n",
       "  'the',\n",
       "  'most',\n",
       "  'was',\n",
       "  'when',\n",
       "  'met',\n",
       "  'you'],\n",
       " ['Tell',\n",
       "  'that',\n",
       "  'to',\n",
       "  'the',\n",
       "  'people',\n",
       "  'operating',\n",
       "  'the',\n",
       "  'live',\n",
       "  'chat',\n",
       "  \"'ve\",\n",
       "  'just',\n",
       "  'spoke',\n",
       "  'to',\n",
       "  'somebody',\n",
       "  'again',\n",
       "  'that',\n",
       "  'said',\n",
       "  'my',\n",
       "  'money',\n",
       "  'will',\n",
       "  'b…'],\n",
       " ['wanted',\n",
       "  'to',\n",
       "  'celebrate',\n",
       "  'reaching',\n",
       "  'followers',\n",
       "  'so',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'ask',\n",
       "  'mates',\n",
       "  'to',\n",
       "  'team',\n",
       "  'up',\n",
       "  'with',\n",
       "  'me',\n",
       "  'for',\n",
       "  '-fold',\n",
       "  'amp',\n",
       "  'whacked',\n",
       "  '£60',\n",
       "  'on',\n",
       "  'i…'],\n",
       " ['was',\n",
       "  'wearing',\n",
       "  'fluffy',\n",
       "  'socks',\n",
       "  'and',\n",
       "  'yeah',\n",
       "  'in',\n",
       "  'the',\n",
       "  'group',\n",
       "  'we',\n",
       "  'have',\n",
       "  'at',\n",
       "  'the',\n",
       "  'moment',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'like',\n",
       "  'boys',\n",
       "  'and',\n",
       "  'girls',\n",
       "  'it',\n",
       "  'was',\n",
       "  'outside',\n",
       "  'hockey'],\n",
       " ['Doing',\n",
       "  'anything',\n",
       "  'this',\n",
       "  'week',\n",
       "  'or',\n",
       "  'next',\n",
       "  'Well',\n",
       "  'my',\n",
       "  'auntie',\n",
       "  'and',\n",
       "  'great',\n",
       "  'nanny',\n",
       "  'are',\n",
       "  'coming',\n",
       "  'down',\n",
       "  'tomorrow',\n",
       "  'untill',\n",
       "  'Thursday',\n",
       "  'the',\n",
       "  '...'],\n",
       " ['feel',\n",
       "  'so',\n",
       "  'sorry',\n",
       "  'for',\n",
       "  'Mitt',\n",
       "  'Romney',\n",
       "  'but',\n",
       "  'sorrier',\n",
       "  'for',\n",
       "  'the',\n",
       "  'country',\n",
       "  'that',\n",
       "  'will',\n",
       "  'never',\n",
       "  'have',\n",
       "  'him',\n",
       "  'as',\n",
       "  'president'],\n",
       " ['to',\n",
       "  'my',\n",
       "  'mother',\n",
       "  'may',\n",
       "  'every',\n",
       "  'tear',\n",
       "  'that',\n",
       "  'has',\n",
       "  'fallen',\n",
       "  'from',\n",
       "  'your',\n",
       "  'tired',\n",
       "  'eyes',\n",
       "  'on',\n",
       "  'my',\n",
       "  'behalf',\n",
       "  'become',\n",
       "  'river',\n",
       "  'for',\n",
       "  'you',\n",
       "  'in',\n",
       "  'Paradise️'],\n",
       " ['haha',\n",
       "  'good',\n",
       "  'idea',\n",
       "  'but',\n",
       "  'am',\n",
       "  'the',\n",
       "  'worse',\n",
       "  'singer',\n",
       "  'on',\n",
       "  'the',\n",
       "  'planet',\n",
       "  'earth',\n",
       "  'can',\n",
       "  'play',\n",
       "  'the',\n",
       "  'kazoo',\n",
       "  'though'],\n",
       " ['Last',\n",
       "  'night',\n",
       "  'we',\n",
       "  'met',\n",
       "  'very',\n",
       "  'brave',\n",
       "  'little',\n",
       "  'dude',\n",
       "  'called',\n",
       "  'Gavin',\n",
       "  'Gavin',\n",
       "  'in',\n",
       "  'an',\n",
       "  'old',\n",
       "  'head',\n",
       "  'on',\n",
       "  'young',\n",
       "  'shoulders',\n",
       "  'so',\n",
       "  'much',\n",
       "  'fun',\n",
       "  'teaching',\n",
       "  'u…'],\n",
       " ['national',\n",
       "  'anthem',\n",
       "  'is',\n",
       "  'also',\n",
       "  'very',\n",
       "  'sensual',\n",
       "  '``',\n",
       "  'sing',\n",
       "  'the',\n",
       "  'national',\n",
       "  'anthem',\n",
       "  'while',\n",
       "  \"'m\",\n",
       "  'standing',\n",
       "  'over',\n",
       "  'your',\n",
       "  'body',\n",
       "  'hold',\n",
       "  'you',\n",
       "  'like',\n",
       "  'python',\n",
       "  \"''\"],\n",
       " ['Family',\n",
       "  'food',\n",
       "  'games',\n",
       "  'ear',\n",
       "  'defenders',\n",
       "  'My',\n",
       "  'autistic',\n",
       "  'son',\n",
       "  'does',\n",
       "  \"n't\",\n",
       "  'mind',\n",
       "  'groups',\n",
       "  'as',\n",
       "  'long',\n",
       "  'as',\n",
       "  'he',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'hear',\n",
       "  'them',\n",
       "  'there',\n",
       "  'is',\n",
       "  'food'],\n",
       " ['Yes',\n",
       "  'this',\n",
       "  'tweet',\n",
       "  'might',\n",
       "  'have',\n",
       "  ',000',\n",
       "  'retweets',\n",
       "  'but',\n",
       "  'one',\n",
       "  'of',\n",
       "  'them',\n",
       "  'is',\n",
       "  'chandler',\n",
       "  'and',\n",
       "  'he',\n",
       "  'never',\n",
       "  'wins',\n",
       "  'So',\n",
       "  'go',\n",
       "  'retweet',\n",
       "  'if',\n",
       "  'you',\n",
       "  'haven',\n",
       "  'caus…'],\n",
       " ['Do',\n",
       "  'you',\n",
       "  'have',\n",
       "  'large',\n",
       "  'crew',\n",
       "  'on-site',\n",
       "  'completing',\n",
       "  'project',\n",
       "  'Make',\n",
       "  'sure',\n",
       "  'to',\n",
       "  'call',\n",
       "  'Andy',\n",
       "  'Gump',\n",
       "  'at',\n",
       "  '-992-7755',\n",
       "  'to',\n",
       "  'provide',\n",
       "  'the',\n",
       "  '...'],\n",
       " ['This',\n",
       "  'guy',\n",
       "  'probably',\n",
       "  'realized',\n",
       "  'what',\n",
       "  'gem',\n",
       "  'of',\n",
       "  'woman',\n",
       "  'he',\n",
       "  'was',\n",
       "  'about',\n",
       "  'to',\n",
       "  'lose',\n",
       "  'so',\n",
       "  'being',\n",
       "  'little',\n",
       "  'fuckboy',\n",
       "  'to',\n",
       "  'save',\n",
       "  'himself',\n",
       "  'th…'],\n",
       " ['remember',\n",
       "  'when',\n",
       "  'Rupaul',\n",
       "  'at',\n",
       "  'season',\n",
       "  \"'s\",\n",
       "  'finale',\n",
       "  'said',\n",
       "  'to',\n",
       "  'Monica',\n",
       "  'Beverly',\n",
       "  'Hillz',\n",
       "  'that',\n",
       "  'to',\n",
       "  'be',\n",
       "  'America',\n",
       "  \"'s\",\n",
       "  'next',\n",
       "  'drag',\n",
       "  'super',\n",
       "  'star…'],\n",
       " ['what',\n",
       "  'do',\n",
       "  'Darren',\n",
       "  'Helm',\n",
       "  'and',\n",
       "  'Thomas',\n",
       "  'Stuart',\n",
       "  'Dant',\n",
       "  'have',\n",
       "  'in',\n",
       "  'common',\n",
       "  'Only',\n",
       "  'members',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Jr',\n",
       "  'Selkirk',\n",
       "  'Fishermen',\n",
       "  'to',\n",
       "  'sign',\n",
       "  'pro',\n",
       "  'deals'],\n",
       " ['Just',\n",
       "  'start',\n",
       "  'doing',\n",
       "  'what',\n",
       "  'you',\n",
       "  'know',\n",
       "  'is',\n",
       "  'the',\n",
       "  'right',\n",
       "  'thing',\n",
       "  'to',\n",
       "  'do',\n",
       "  'and',\n",
       "  'the',\n",
       "  'feelings',\n",
       "  'will',\n",
       "  'follow',\n",
       "  'Do',\n",
       "  \"n't\",\n",
       "  'wait.Action',\n",
       "  'ignites',\n",
       "  'motivation'],\n",
       " ['heard',\n",
       "  'sirens',\n",
       "  'in',\n",
       "  'Matlock',\n",
       "  'area',\n",
       "  'mid',\n",
       "  'afternoon',\n",
       "  'an',\n",
       "  'hour',\n",
       "  'or',\n",
       "  'so',\n",
       "  'later',\n",
       "  'Hercules',\n",
       "  'flying',\n",
       "  'back',\n",
       "  'and',\n",
       "  'forth',\n",
       "  'along',\n",
       "  'Dunnottar',\n",
       "  'shoreline'],\n",
       " ['Your',\n",
       "  'favorite',\n",
       "  'teacher',\n",
       "  'band',\n",
       "  'will',\n",
       "  'be',\n",
       "  'playing',\n",
       "  'at',\n",
       "  ':45',\n",
       "  'tomorrow',\n",
       "  'night',\n",
       "  'at',\n",
       "  'Loosey',\n",
       "  \"'s\",\n",
       "  'Open',\n",
       "  'Mic',\n",
       "  'Night',\n",
       "  'Come',\n",
       "  'out',\n",
       "  'and',\n",
       "  'show',\n",
       "  'some',\n",
       "  'love'],\n",
       " ['Yesterday',\n",
       "  'was',\n",
       "  'the',\n",
       "  'best',\n",
       "  'day',\n",
       "  'of',\n",
       "  'my',\n",
       "  'LIFE',\n",
       "  'Took',\n",
       "  'my',\n",
       "  'Shahada',\n",
       "  'and',\n",
       "  'became',\n",
       "  'Muslim',\n",
       "  'want',\n",
       "  'to',\n",
       "  'thank',\n",
       "  'everyone',\n",
       "  'that',\n",
       "  'was',\n",
       "  'apart',\n",
       "  'of',\n",
       "  'this…'],\n",
       " ['omg',\n",
       "  'aht',\n",
       "  'time',\n",
       "  'my',\n",
       "  'boyfriend',\n",
       "  'is',\n",
       "  'soo',\n",
       "  'needy',\n",
       "  'just',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'punch',\n",
       "  'home',\n",
       "  'but',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'help',\n",
       "  'it',\n",
       "  'but',\n",
       "  'to',\n",
       "  'love',\n",
       "  'him'],\n",
       " ['Micky',\n",
       "  'giving',\n",
       "  'you',\n",
       "  'small',\n",
       "  'locket',\n",
       "  'before',\n",
       "  'leaving',\n",
       "  'to',\n",
       "  'tour',\n",
       "  'the',\n",
       "  'picture',\n",
       "  'inside',\n",
       "  'is',\n",
       "  'of',\n",
       "  'you',\n",
       "  'two',\n",
       "  'the',\n",
       "  'day',\n",
       "  'you',\n",
       "  'met'],\n",
       " ['have',\n",
       "  'just',\n",
       "  'small',\n",
       "  'desk',\n",
       "  'on',\n",
       "  'one',\n",
       "  'wall',\n",
       "  'for',\n",
       "  'my',\n",
       "  'old',\n",
       "  'computer',\n",
       "  'twin',\n",
       "  'bed',\n",
       "  'parked',\n",
       "  'against',\n",
       "  'the',\n",
       "  'far',\n",
       "  'wall'],\n",
       " ['told',\n",
       "  'my',\n",
       "  'cat',\n",
       "  'she',\n",
       "  'wasn',\n",
       "  'gon',\n",
       "  'na',\n",
       "  'like',\n",
       "  'this',\n",
       "  'salt',\n",
       "  'and',\n",
       "  'vinegar',\n",
       "  'chip',\n",
       "  'but',\n",
       "  'she',\n",
       "  'didn',\n",
       "  'listen',\n",
       "  'to',\n",
       "  'me',\n",
       "  'smh',\n",
       "  'SHE',\n",
       "  'GAGGED'],\n",
       " ['dont',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'go',\n",
       "  'to',\n",
       "  'school',\n",
       "  'but',\n",
       "  'think',\n",
       "  'might',\n",
       "  'have',\n",
       "  'to',\n",
       "  '...',\n",
       "  'so',\n",
       "  'much',\n",
       "  'on',\n",
       "  'my',\n",
       "  'mind',\n",
       "  'cant',\n",
       "  'deal'],\n",
       " ['So',\n",
       "  'Elizabeth',\n",
       "  'Warren',\n",
       "  'has',\n",
       "  'the',\n",
       "  'stamina',\n",
       "  'to',\n",
       "  'take',\n",
       "  'all',\n",
       "  'those',\n",
       "  'selfies',\n",
       "  'and',\n",
       "  'still',\n",
       "  'have',\n",
       "  'energy',\n",
       "  'to',\n",
       "  'have',\n",
       "  'an',\n",
       "  'affair',\n",
       "  'with',\n",
       "  'year',\n",
       "  'old…'],\n",
       " ['If',\n",
       "  'bride',\n",
       "  'does',\n",
       "  \"n't\",\n",
       "  'wear',\n",
       "  'false',\n",
       "  'eyelashes',\n",
       "  'at',\n",
       "  'her',\n",
       "  'wedding',\n",
       "  'the',\n",
       "  'marriage',\n",
       "  'will',\n",
       "  'be',\n",
       "  'complete',\n",
       "  'amp',\n",
       "  'utter',\n",
       "  'failure'],\n",
       " ['Micky',\n",
       "  'and',\n",
       "  'you',\n",
       "  'cooking',\n",
       "  'it',\n",
       "  'actually',\n",
       "  'comes',\n",
       "  'out',\n",
       "  'really',\n",
       "  'well',\n",
       "  'but',\n",
       "  'when',\n",
       "  'your',\n",
       "  'finally',\n",
       "  'finished',\n",
       "  'you',\n",
       "  'both',\n",
       "  'are',\n",
       "  \"n't\",\n",
       "  'hungry',\n",
       "  'anymore'],\n",
       " ['It',\n",
       "  \"'s\",\n",
       "  'Friday',\n",
       "  'and',\n",
       "  'time',\n",
       "  'to',\n",
       "  'plan',\n",
       "  'relaxing',\n",
       "  'weekend',\n",
       "  'Try',\n",
       "  'Having',\n",
       "  'lie-in',\n",
       "  'Getting',\n",
       "  'some',\n",
       "  'fresh',\n",
       "  'air',\n",
       "  'in',\n",
       "  'the',\n",
       "  'house…'],\n",
       " ['Tell',\n",
       "  'lay',\n",
       "  'said',\n",
       "  'HELLO',\n",
       "  'amp',\n",
       "  'Dede',\n",
       "  'is',\n",
       "  'she',\n",
       "  'chill',\n",
       "  'now',\n",
       "  'that',\n",
       "  'she',\n",
       "  'got',\n",
       "  'some',\n",
       "  'weed',\n",
       "  'in',\n",
       "  'her',\n",
       "  'system',\n",
       "  'But',\n",
       "  \"'m\",\n",
       "  'in',\n",
       "  'palm',\n",
       "  'desert',\n",
       "  'again'],\n",
       " ['Government',\n",
       "  'plans',\n",
       "  'will',\n",
       "  'starve',\n",
       "  'injured',\n",
       "  'people',\n",
       "  'of',\n",
       "  'access',\n",
       "  'to',\n",
       "  'justice',\n",
       "  'and',\n",
       "  'take',\n",
       "  'at',\n",
       "  'least',\n",
       "  '£135m',\n",
       "  'from',\n",
       "  'public',\n",
       "  'funds',\n",
       "  'Act',\n",
       "  'now',\n",
       "  'ht…'],\n",
       " [\"'no\",\n",
       "  'Dan',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'want',\n",
       "  'to',\n",
       "  'go',\n",
       "  'on',\n",
       "  'this',\n",
       "  'one',\n",
       "  'you',\n",
       "  'scream',\n",
       "  'at',\n",
       "  'him',\n",
       "  'as',\n",
       "  'he',\n",
       "  'drags',\n",
       "  'you',\n",
       "  'in',\n",
       "  'the',\n",
       "  'short',\n",
       "  'queue',\n",
       "  'of',\n",
       "  'the',\n",
       "  'cont']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 16816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total converted: 16816\n",
      "Total sentences: 4205\n",
      "Total converted: 4205\n"
     ]
    }
   ],
   "source": [
    "X_train_avg = word_averaging_sentences(wv, X_train_tokenized)\n",
    "X_test_avg = word_averaging_sentences(wv, X_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5443519619500594\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='sag', max_iter=1000)\n",
    "logreg = logreg.fit(X_train_avg, Y_train)\n",
    "y_pred = logreg.predict(X_test_avg)\n",
    "print('accuracy %s' % accuracy_score(y_pred, Y_test))\n",
    "#print(classification_report(Y_test, y_pred, target_names=['female', 'male']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5267538644470868"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVC()\n",
    "\n",
    "svm_model.fit(X_train_avg, Y_train)\n",
    "\n",
    "predictions = svm_model.predict(X_test_avg)\n",
    "\n",
    "precision_recall_fscore_support(Y_test, predictions)\n",
    "\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reusable code from Analytics Vidya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False, batch_size=32):\n",
    "    # fit the training dataset on the classifier\n",
    "    if is_neural_net == False:\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "    else:\n",
    "        classifier.fit(feature_vector_train, label, batch_size=batch_size)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = [int(round(p[0])) for p in predictions]\n",
    "    \n",
    "    return metrics.accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest on Word Level TF IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF:  0.48466111771700354\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(ensemble.RandomForestClassifier(), X_train_avg, Y_train, X_test_avg)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.5239001189060642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvashisth\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(xgboost.XGBClassifier(), X_train_avg, Y_train, X_test_avg)\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
